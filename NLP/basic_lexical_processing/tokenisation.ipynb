{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365ffa1f-8efe-4304-838b-29421ca78019",
   "metadata": {},
   "source": [
    "# Tokenisation\n",
    "\n",
    "- <b>Definition:</b> Tokenization is the process of breaking down a stream of text into smaller units called \"tokens.\" These tokens can be words, phrases, or other meaningful sub-units like punctuation or special characters depending on the application type we are working on.\n",
    "\n",
    "- <b>Purpose:</b> Tokenization simplifies text processing by converting unstructured text into structured, analyzable components.\n",
    "\n",
    "- <b>Types:</b> Word Tokenisation, Sentence Tokenisation, Tweet Tokenisation, Subword Tokenisation, N-gram Tokenisation, Regex-based Tokenisation, Whitespace Tokenisation, etc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e222c6-b359-4613-97b2-c4fad652d00f",
   "metadata": {},
   "source": [
    "#### We will be using NLTK library for basic tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d86b38-dc51-40dd-8154-7550679e532b",
   "metadata": {},
   "source": [
    "### 1. Word Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bdd61db-48ea-49ab-b05f-c8c04ab05341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\n"
     ]
    }
   ],
   "source": [
    "document = \"At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ade1949-34e9-4a1e-ba55-1a701d19b06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself.', 'It', 'looks', 'like', 'religious', 'mania,', 'and', \"he'll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenise the above document into words using simple python split method\n",
    "print(document.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21a078a-9950-41e7-be6b-d4daa23d2c7a",
   "metadata": {},
   "source": [
    "#### However, the above method has just split on white space which left us with words like God with a period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "820e7c65-2575-4386-a811-e52d10c0e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nehaverma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/nehaverma/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us download punkt and punkt_tab, which is a pre-trained tokenization model provided by the Natural Language Toolkit (NLTK). It is specifically designed to handle sentence and word tokenization for a variety of languages.\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c3907a-9663-4c8c-97ce-a077761890a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'nine', \"o'clock\", 'I', 'visited', 'him', 'myself', '.', 'It', 'looks', 'like', 'religious', 'mania', ',', 'and', 'he', \"'ll\", 'soon', 'think', 'that', 'he', 'himself', 'is', 'God', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenise using nltk library\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de684cfb-208e-4c2c-b6cd-ac420731c3ef",
   "metadata": {},
   "source": [
    "#### As we now see, the words in the document using nltk is tokenized not only on white space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42cafa9-aef0-48dc-8d4f-e162ea64ed82",
   "metadata": {},
   "source": [
    "### 2. Sentence tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3f0d275-cef8-46f1-8a93-e9a93b18f6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At nine o'clock I visited him myself.\", \"It looks like religious mania, and he'll soon think that he himself is God.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "print(sent_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "914bf1d9-3c5b-47fe-a64b-b4bbb9522e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello World!', 'Are you enjoying Data Science?', 'I hope you are.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize('Hello World! Are you enjoying Data Science? I hope you are.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb00506-3629-4a17-ba2d-a2188a0b4cfb",
   "metadata": {},
   "source": [
    "### 3. Tweet tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2691a018-0983-47ba-b4f9-d854d5427f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"i recently watched this show called mindhunters:). i totally loved it üòç. it was gr8 <3. #bingewatching #nothingtodo üòé\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89e98ae9-a104-41ef-9855-9ff2d81e8322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':', ')', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<', '3', '.', '#', 'bingewatching', '#', 'nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "# Lets try using word tokenizer for above message\n",
    "\n",
    "print(word_tokenize(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba5efd5-6de9-48f4-a480-07983a9ca1ad",
   "metadata": {},
   "source": [
    "A problem with word tokeniser is that it fails to tokeniser emojis and other complex special characters such as word with hashtags. Emojis are common these days and people use them all the time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27dda7-6467-468d-af54-521a1a4b44b0",
   "metadata": {},
   "source": [
    "The word tokeniser breaks the emoji '<3' into '<' and '3' which is something that we don't want. Emojis have their own significance in areas like sentiment analysis where a happy face and sad face can salone prove to be a really good predictor of the sentiment. Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook. So there, you want to use the hashtag as is.\n",
    "\n",
    "Let's use the tweet tokeniser of nltk to tokenise this message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7a59359-4c27-4528-a5d0-3eec20380631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ccda4ff-cdd3-4ac9-8bc7-fa1641a29a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'recently', 'watched', 'this', 'show', 'called', 'mindhunters', ':)', '.', 'i', 'totally', 'loved', 'it', 'üòç', '.', 'it', 'was', 'gr8', '<3', '.', '#bingewatching', '#nothingtodo', 'üòé']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(message))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bece98-113b-4220-aa36-17788408d95b",
   "metadata": {},
   "source": [
    "As we can see, it handles all the emojis and the hashtags pretty well.\n",
    "\n",
    "Now, there is a tokeniser that takes a regular expression and tokenises and returns result based on the pattern of regular expression.\n",
    "\n",
    "Let's look at how you can use regular expression tokeniser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc0dd2b-215a-4eac-80bd-24e75d131f2d",
   "metadata": {},
   "source": [
    "## 4. Regex Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78e1940e-d2d6-4b19-86c6-9492634ae7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d30bd038-cdfb-43a9-ad38-b4b4a0153730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d64a58af-5483-47de-9580-0511369f72d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#bingewatching', '#nothingtodo']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us filter all hastags from the above message using regexp_tokenize\n",
    "\n",
    "pattern = '#[\\w]+'\n",
    "\n",
    "regexp_tokenize(message, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097395c7-13f4-4ffc-9dcf-fe3fc125d633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
