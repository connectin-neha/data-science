{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38f56815-29a3-4ed1-b7f6-2f9edc610b75",
   "metadata": {},
   "source": [
    "# Lemmatisation\n",
    "\n",
    "This is a more sophisticated technique (and perhaps more 'intelligent') in the sense that it doesn’t just chop off the suffix of a word. Instead, it takes an input word and searches for its base word by going recursively through all the variations of dictionary words. The base word in this case is called the lemma. Words such as ‘feet’, ‘drove’, ‘arose’, ‘bought’, etc. can’t be reduced to their correct base form using a stemmer. But a lemmatizer can reduce them to their correct base form. The most popular lemmatizer is the WordNet lemmatizer created by a team od researchers at the Princeton university. We can read more about it <a href=\"https://wordnet.princeton.edu/\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "842456cf-5b6c-4050-a865-0593b3a1f895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required librarires\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458620b4-ad33-4420-8674-6cafdf671896",
   "metadata": {},
   "source": [
    "#### Let us do lemmatisation of sample corpus text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47cdcef0-7a09-4f1f-9797-e036d670d681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\n"
     ]
    }
   ],
   "source": [
    "text = \"Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6306f5c-1b8c-40b4-97cc-5ecc91eb1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens =  word_tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df590788-e488-4142-8f98-4f79b1adc480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['very', 'orderly', 'and', 'methodical', 'he', 'looked', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'ticking', 'a', 'sonorous', 'sermon', 'under', 'his', 'flapped', 'newly', 'bought', 'waist-coat', ',', 'as', 'though', 'it', 'pitted', 'its', 'gravity', 'and', 'longevity', 'against', 'the', 'levity', 'and', 'evanescence', 'of', 'the', 'brisk', 'fire', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d27421-4b07-4610-9b1c-40606f86572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nehaverma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the lemmas\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae43aed7-2ca4-4903-a916-1ce5a6b235a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['very', 'orderly', 'and', 'methodical', 'he', 'looked', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'ticking', 'a', 'sonorous', 'sermon', 'under', 'his', 'flapped', 'newly', 'bought', 'waist-coat', ',', 'a', 'though', 'it', 'pitted', 'it', 'gravity', 'and', 'longevity', 'against', 'the', 'levity', 'and', 'evanescence', 'of', 'the', 'brisk', 'fire', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatize the tokens\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2953f8f-b666-4a1c-b189-632f50ba32ef",
   "metadata": {},
   "source": [
    "### Lemmatisation v/s Stemming\n",
    "\n",
    "- A stemmer is a rule based technique, and hence, it is much faster than the lemmatizer (which searches the dictionary to look for the lemma of a word). On the other hand, a stemmer typically gives less accurate results than a lemmatizer.\n",
    "\n",
    "- A lemmatizer is slower because of the dictionary lookup but gives better results than a stemmer. It is important to know that for a lemmatizer to perform accurately, we need to provide the part-of-speech tag of the input word (noun, verb, adjective etc.)But there are often cases when the POS tagger itself is quite inaccurate on our text, and that will worsen the performance of the lemmatiser as well. In short, we may want to consider a stemmer rather than a lemmatiser if we notice that POS tagging is inaccurate.\n",
    "\n",
    "- Also, Lemmatization only works on correctly spelt words. While stemming is rule based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62c93cb2-1164-48e8-b59f-2cf7ac1f2e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['veri', 'order', 'and', 'method', 'he', 'look', ',', 'with', 'a', 'hand', 'on', 'each', 'knee', ',', 'and', 'a', 'loud', 'watch', 'tick', 'a', 'sonor', 'sermon', 'under', 'his', 'flap', 'newli', 'bought', 'waist-coat', ',', 'as', 'though', 'it', 'pit', 'it', 'graviti', 'and', 'longev', 'against', 'the', 'leviti', 'and', 'evanesc', 'of', 'the', 'brisk', 'fire', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "stemmed = [stemmer.stem(token) for token in tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea4d6c14-0947-4f23-b6e7-9c3f57f1ba1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "token_df = pd.DataFrame({'token': tokens, 'lemmatized': lemmatized, 'stemmed':stemmed})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a18997f-8e40-4871-908a-ee040a22e87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>very</td>\n",
       "      <td>very</td>\n",
       "      <td>veri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orderly</td>\n",
       "      <td>orderly</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>methodical</td>\n",
       "      <td>methodical</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token  lemmatized stemmed\n",
       "0        very        very    veri\n",
       "1     orderly     orderly   order\n",
       "2         and         and     and\n",
       "3  methodical  methodical  method\n",
       "4          he          he      he"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96fa355a-359e-452b-be36-b5915689e83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_token_df = token_df[(token_df.lemmatized != token_df.token) | (token_df.stemmed != token_df.token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b09e8a3-5314-4247-bbc6-ae047f7aed94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_token_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f458af-1e4b-4c6b-8bb9-cabb37e9439e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>very</td>\n",
       "      <td>very</td>\n",
       "      <td>veri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>orderly</td>\n",
       "      <td>orderly</td>\n",
       "      <td>order</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>methodical</td>\n",
       "      <td>methodical</td>\n",
       "      <td>method</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>looked</td>\n",
       "      <td>looked</td>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ticking</td>\n",
       "      <td>ticking</td>\n",
       "      <td>tick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>sonorous</td>\n",
       "      <td>sonorous</td>\n",
       "      <td>sonor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>flapped</td>\n",
       "      <td>flapped</td>\n",
       "      <td>flap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>newly</td>\n",
       "      <td>newly</td>\n",
       "      <td>newli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>as</td>\n",
       "      <td>a</td>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>pitted</td>\n",
       "      <td>pitted</td>\n",
       "      <td>pit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>its</td>\n",
       "      <td>it</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gravity</td>\n",
       "      <td>gravity</td>\n",
       "      <td>graviti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>longevity</td>\n",
       "      <td>longevity</td>\n",
       "      <td>longev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>levity</td>\n",
       "      <td>levity</td>\n",
       "      <td>leviti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>evanescence</td>\n",
       "      <td>evanescence</td>\n",
       "      <td>evanesc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          token   lemmatized  stemmed\n",
       "0          very         very     veri\n",
       "1       orderly      orderly    order\n",
       "3    methodical   methodical   method\n",
       "5        looked       looked     look\n",
       "18      ticking      ticking     tick\n",
       "20     sonorous     sonorous    sonor\n",
       "24      flapped      flapped     flap\n",
       "25        newly        newly    newli\n",
       "29           as            a       as\n",
       "32       pitted       pitted      pit\n",
       "33          its           it       it\n",
       "34      gravity      gravity  graviti\n",
       "36    longevity    longevity   longev\n",
       "39       levity       levity   leviti\n",
       "41  evanescence  evanescence  evanesc"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_token_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c91a3-fb49-4a31-a058-658c6e759ee8",
   "metadata": {},
   "source": [
    "#### As we can see there are some words lemmatized incorrectly. That's because we also need to pass the POS (part of speech) for correcting this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5383e2b4-bd11-4dd6-8883-33e297e0d7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'working'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example, let us lemmatize the word working without passing POS\n",
    "wordnet_lemmatizer.lemmatize('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "610e7d1a-9c12-4129-9cb6-31e277b4c28c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now when we pass POD as verb (v), it lemmatizes it correctly\n",
    "wordnet_lemmatizer.lemmatize('working', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e8946-640f-4e50-83d7-13546b64aa45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
