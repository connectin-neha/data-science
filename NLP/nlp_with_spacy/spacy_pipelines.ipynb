{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6276baf-93d1-45e5-9802-2461d4346999",
   "metadata": {},
   "source": [
    "# spaCy Pipelines\n",
    "\n",
    "A pipeline is a sequence of pipes (pipeline components), or actors on data, that make alterations to the data or extract information from it. In some cases, later pipes require the output from earlier components, while in other cases, a pipe can exist entirely on its own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04649dbc-74dc-4eb8-b631-0a3d11815e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9f5a626-d479-4a0b-bdd0-5e8a96796770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load english small model to text processing from spaCy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28eb1723-e37b-4a5d-a7a1-f4acc922f2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us create a spaCy pipeline to basic processing on a sample text\n",
    "doc = nlp('We are learning spaCy pipelines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d99d80-30de-4ab7-9385-533ec7bb0b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49928f88-9506-40a9-93d5-567484fc6328",
   "metadata": {},
   "source": [
    "When we call nlp on a text, spaCy first tokenizes the text to produce a Doc container. The Doc object is then processed in several different steps, known as the processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1249eaee-ca4d-4d50-88ba-046b3f27ef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'learning', 'spaCy', 'pipelines']\n"
     ]
    }
   ],
   "source": [
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc0117b7-385b-4adb-8e1c-ae92a135e46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRON'), ('are', 'AUX'), ('learning', 'VERB'), ('spaCy', 'NUM'), ('pipelines', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print([(token.text,token.pos_) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a08de4f-4fd4-483a-a2a3-e6bb0b3e6105",
   "metadata": {},
   "source": [
    "Another example, for a named entity recognition pipeline, three pipes can be used: a Tokenizer pipe, which is the first processing step in spaCy pipelines; a rule-based named entity recognizer known as the EntityRuler, which finds entities; and an EntityLinker pipe that identifies the type of each entity. Through this processing pipeline, an input text is converted to a Doc container with its corresponding annotated entities. We can use the doc-dot-ents feature to find the entities in the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47bc6bab-e02f-4a7b-a7c8-903213699525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Albert Einstein', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Albert Einstein was genius')\n",
    "\n",
    "print([(ent.text,ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ddddc0-0b84-4614-bd4a-6777a1e0a8a2",
   "metadata": {},
   "source": [
    "#### Adding Pipes\n",
    "\n",
    "We often use an existing spaCy model. However, in some cases, an off-the-shelf model will not satisfy our requirements. Hence we need to add custom pipes in such cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82758fd0-14ab-4211-874a-09d78e34b167",
   "metadata": {},
   "source": [
    "An example of this is the sentence segmentation for a long document with 10,000 sentences. Even if we use the smallest English model, the most efficient spaCy model, en_core_web_sm, the model can take a long time to process 10,000 sentences and separate them. The reason is that when calling an existing spaCy model on a text, the whole NLP pipeline will be activated and that means that each pipe from named entity recognition to dependency parsing will run on the text. This increases the use of computational time by 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12421d8-bef1-4384-8d96-d20eecbfff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6ce9219-7093-42cb-a0c3-5597dcece842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing with en_core_web_sm model in 0.22787 minutes\n"
     ]
    }
   ],
   "source": [
    "text = ' '.join(['This is a sample test sentence.']*10000)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Finished processing with en_core_web_sm model in {0} minutes'.format(round((end_time-start_time)/60.0, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a113372-f785-44ef-9709-9783a4f59b41",
   "metadata": {},
   "source": [
    "In this instance, we would want to make a blank spaCy English model by using spacy.blank(\"en\") and add the sentencizer component to the pipeline by using .add_pipe() method of the nlp model. \n",
    "\n",
    "By creating a blank model and simply adding a sentencizer pipe, we can considerably reduce computational time. The reason is that for this version of the spaCy model, only intended pipeline component (sentence segmentation) will run on the given documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a38065d-6aca-4424-bfcc-048537a38a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing with en_core_web_sm model in 0.00206 minutes\n"
     ]
    }
   ],
   "source": [
    "# Let us see this\n",
    "\n",
    "blank_nlp = spacy.blank('en')\n",
    "blank_nlp.add_pipe('sentencizer')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "doc = blank_nlp(text)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print('Finished processing with en_core_web_sm model in {0} minutes'.format(round((end_time-start_time)/60.0, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711161f7-7931-4e24-9ce2-a56840701acb",
   "metadata": {},
   "source": [
    "As we see the computational time is considerably reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c777f5-ec2f-4249-bd05-71df10f07525",
   "metadata": {},
   "source": [
    "#### Analyzing pipeline components\n",
    "\n",
    "spaCy allows us to analyze a spaCy pipeline to check whether any required attributes are not set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "092c49d6-7757-4818-bf04-e9694ef0b5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component         Assigns               Requires   Scores             Retokenizes\n",
      "-   ---------------   -------------------   --------   ----------------   -----------\n",
      "0   tok2vec           doc.tensor                                          False      \n",
      "                                                                                     \n",
      "1   tagger            token.tag                        tag_acc            False      \n",
      "                                                                                     \n",
      "2   parser            token.dep                        dep_uas            False      \n",
      "                      token.head                       dep_las                       \n",
      "                      token.is_sent_start              dep_las_per_type              \n",
      "                      doc.sents                        sents_p                       \n",
      "                                                       sents_r                       \n",
      "                                                       sents_f                       \n",
      "                                                                                     \n",
      "3   attribute_ruler                                                       False      \n",
      "                                                                                     \n",
      "4   lemmatizer        token.lemma                      lemma_acc          False      \n",
      "                                                                                     \n",
      "5   ner               doc.ents                         ents_f             False      \n",
      "                      token.ent_iob                    ents_p                        \n",
      "                      token.ent_type                   ents_r                        \n",
      "                                                       ents_per_type                 \n",
      "\n",
      "\u001b[38;5;2mâœ” No problems found.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'tok2vec': {'assigns': ['doc.tensor'],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'tagger': {'assigns': ['token.tag'],\n",
       "   'requires': [],\n",
       "   'scores': ['tag_acc'],\n",
       "   'retokenizes': False},\n",
       "  'parser': {'assigns': ['token.dep',\n",
       "    'token.head',\n",
       "    'token.is_sent_start',\n",
       "    'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['dep_uas',\n",
       "    'dep_las',\n",
       "    'dep_las_per_type',\n",
       "    'sents_p',\n",
       "    'sents_r',\n",
       "    'sents_f'],\n",
       "   'retokenizes': False},\n",
       "  'attribute_ruler': {'assigns': [],\n",
       "   'requires': [],\n",
       "   'scores': [],\n",
       "   'retokenizes': False},\n",
       "  'lemmatizer': {'assigns': ['token.lemma'],\n",
       "   'requires': [],\n",
       "   'scores': ['lemma_acc'],\n",
       "   'retokenizes': False},\n",
       "  'ner': {'assigns': ['doc.ents', 'token.ent_iob', 'token.ent_type'],\n",
       "   'requires': [],\n",
       "   'scores': ['ents_f', 'ents_p', 'ents_r', 'ents_per_type'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'tok2vec': [],\n",
       "  'tagger': [],\n",
       "  'parser': [],\n",
       "  'attribute_ruler': [],\n",
       "  'lemmatizer': [],\n",
       "  'ner': []},\n",
       " 'attrs': {'doc.ents': {'assigns': ['ner'], 'requires': []},\n",
       "  'doc.tensor': {'assigns': ['tok2vec'], 'requires': []},\n",
       "  'token.lemma': {'assigns': ['lemmatizer'], 'requires': []},\n",
       "  'token.dep': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.head': {'assigns': ['parser'], 'requires': []},\n",
       "  'doc.sents': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.tag': {'assigns': ['tagger'], 'requires': []},\n",
       "  'token.ent_type': {'assigns': ['ner'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['parser'], 'requires': []},\n",
       "  'token.ent_iob': {'assigns': ['ner'], 'requires': []}}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219efc4-2525-42d5-bd61-78b6dfb942d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
