{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "763e248d-5b92-4a58-bb71-7e25cbcb0404",
   "metadata": {},
   "source": [
    "# Customizing spaCy Model\n",
    "\n",
    "spaCy model if required can be trained to meet our domain requirements if the existing model is unable to meet our needs\n",
    "\n",
    "<b>Trainings steps</b>\n",
    "\n",
    "  1. Annotate and prepare input data\n",
    "  2. Initialize the model weight\n",
    "  3. Predict a few examples with current weights\n",
    "  4. Compare prediction with correct answers\n",
    "  5. Use optimizer to calculate weights that improve model performance\n",
    "  6. Update weights slightly\n",
    "  7. Go back to step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822c30a6-d8b8-40f5-b8e3-036d967f6c79",
   "metadata": {},
   "source": [
    "#### 1. Annotating and preparing data\n",
    "\n",
    "* First step is to prepare training data in required format\n",
    "* After collecting data, we annotate it\n",
    "* Annotation means labeling the intent, entities, etc.\\\n",
    "* This is an example of annotated data\n",
    "\n",
    "  `annotated_data = {`\n",
    "  <br>`\"sentence\": \"An antivirul drugs used against influenza is neuraminidase inhibitors.\",`\n",
    "  <br>`\"entities\": {`\n",
    "  <br>              `\"label\": \"Medicine\"`,\n",
    "  <br>              `\"value\": \"neuraminidase inhibitors\",`\n",
    "  <br>`}`\n",
    "  <br>`}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383cf8fd-bfa5-4c52-9917-494ddcef74ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A patient with chest pain had hyperthyroidism.', {'entities': [(15, 25, 'SYMPTOM'), (30, 45, 'DISEASE')]})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"A patient with chest pain had hyperthyroidism.\"\n",
    "entity_1 = \"chest pain\"\n",
    "entity_2 = \"hyperthyroidism\"\n",
    "\n",
    "# Store annotated data information in the correct format\n",
    "annotated_data = {\"sentence\": text, \"entities\": [{\"label\": \"SYMPTOM\", \"value\": entity_1}, {\"label\": \"DISEASE\", \"value\": entity_2}]}\n",
    "\n",
    "# Extract start and end characters of each entity\n",
    "entity_1_start_char = text.find(entity_1)\n",
    "entity_1_end_char = entity_1_start_char + len(entity_1)\n",
    "entity_2_start_char = text.find(entity_2)\n",
    "entity_2_end_char = entity_2_start_char + len(entity_2)\n",
    "\n",
    "# Store the same input information in the proper format for training\n",
    "training_data = [(text, {\"entities\": [(entity_1_start_char,entity_1_end_char,\"SYMPTOM\"), \n",
    "                                      (entity_2_start_char,entity_2_end_char,\"DISEASE\")]})]\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1a53d-ddd1-4ed7-b7e4-8a31e43de1ee",
   "metadata": {},
   "source": [
    "#### Example object data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3fba05e-9586-48fa-b858-81795707725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import spacy\n",
    "from spacy.training import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6225d5-154e-4f71-8030-ef9543a77458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'O', 'O', 'U-GPE'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['I', 'will', 'visit', 'you', 'in', 'Austin'], 'SPACY': [True, True, True, True, True, False], 'TAG': ['', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', ''], 'POS': ['', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5], 'DEP': ['', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0]}}\n"
     ]
    }
   ],
   "source": [
    "# Let us train the model with example sentence\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp('I will visit you in Austin')\n",
    "\n",
    "annotations = {'entities':[(20,26, 'GPE')]}\n",
    "\n",
    "example_sentence = Example.from_dict(doc, annotations)\n",
    "\n",
    "print(example_sentence.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d821f553-fd85-41da-b989-5f4afb743502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'B-SYMPTOM', 'L-SYMPTOM', 'O', 'U-DISEASE', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['A', 'patient', 'with', 'chest', 'pain', 'had', 'hyperthyroidism', '.'], 'SPACY': [True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7], 'DEP': ['', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0]}} \n",
      "\n",
      "Number of formatted training data:  1\n"
     ]
    }
   ],
   "source": [
    "example_text = 'A patient with chest pain had hyperthyroidism.'\n",
    "training_data = [(example_text, {'entities': [(15, 25, 'SYMPTOM'), (30, 45, 'DISEASE')]})]\n",
    "\n",
    "all_examples = []\n",
    "\n",
    "# Iterate through text and annotations and convert text to a Doc container\n",
    "for text, annotations in training_data:\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  # Create an Example object from the doc contianer and annotations\n",
    "  example_sentence = Example.from_dict(doc, annotations)\n",
    "  print(example_sentence.to_dict(), \"\\n\")\n",
    "  \n",
    "  # Append the Example object to the list of all examples\n",
    "  all_examples.append(example_sentence)\n",
    "  \n",
    "print(\"Number of formatted training data: \", len(all_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f7142-055b-49cc-99ef-5cfca67d87f1",
   "metadata": {},
   "source": [
    "#### It is necessary to disable other pipeline components of an nlp model in order to only train the intended component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff09dbb2-a095-4775-9e8a-577f8f97d552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let us disable all other pipes apart from ner\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "nlp.disable_pipes(*other_pipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9188209d-ec19-4584-96ac-74f3397c0432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example object for training: \n",
      " {'doc_annotation': {'cats': {}, 'entities': ['O', 'O', 'O', 'B-SYMPTOM', 'L-SYMPTOM', 'O', 'U-DISEASE', 'O'], 'spans': {}, 'links': {}}, 'token_annotation': {'ORTH': ['A', 'patient', 'with', 'chest', 'pain', 'had', 'hyperthyroidism', '.'], 'SPACY': [True, True, True, True, True, True, False, False], 'TAG': ['', '', '', '', '', '', '', ''], 'LEMMA': ['', '', '', '', '', '', '', ''], 'POS': ['', '', '', '', '', '', '', ''], 'MORPH': ['', '', '', '', '', '', '', ''], 'HEAD': [0, 1, 2, 3, 4, 5, 6, 7], 'DEP': ['', '', '', '', '', '', '', ''], 'SENT_START': [1, 0, 0, 0, 0, 0, 0, 0]}}\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Disable all pipeline components of  except `ner`\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "\n",
    "# Convert a text and its annotations to the correct format usable for training\n",
    "doc = nlp.make_doc(text)\n",
    "example = Example.from_dict(doc, annotations)\n",
    "print(\"Example object for training: \\n\", example.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1899c213-d034-4247-81b6-62a8549b84d4",
   "metadata": {},
   "source": [
    "#### 2. Model Training Procedure\n",
    "\n",
    "- Go over the training set several times; one iteration is called an epoch\n",
    "\n",
    "- In each epoch, the training code updates the weights of the model with a small number using an optimizer object on randomly shuffled training data.\n",
    "\n",
    "- <b>Optimizers</b> update the model weights. Optimizers are functions that update the model weights and aim to lower the risk of errors from these predictions, and improve the accuracy of the model. We can create an optimizer object as below\n",
    "  <br><br>optimizer = nlp.create_optimizer()\n",
    "\n",
    "- In each epoch, we first shuffle training_data, an Example object, by using random.shuffle() method.\n",
    "\n",
    "- Next, for each training data point, which is a tuple of a text and annotations, we extract the equivalent dictionary object from the Example object given the Doc container of a text and training data annotation using Example.from_dict() method.\n",
    "\n",
    "- The extracted Example dictionary will be used to update the nlp model weights by using the nlp.update() method and passing the list of the example dictionary, the optimizer object and a losses dictionary to track model's loss during training. Loss is a number indicating how bad the model's prediction is on a single example.\n",
    "\n",
    "- The procedure continues to process next training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82af0e04-d257-407f-921c-039df19a7ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:  [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n",
      "After training:  [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Test data for before/after training\n",
    "test = \"Apple is looking at buying a U.K. startup for $1 billion.\"\n",
    "\n",
    "# Training data (replace with your actual training data)\n",
    "training_data = [\n",
    "    (\"Google was founded in 1998.\", {\"entities\": [(0, 6, \"ORG\")]}),\n",
    "    (\"Apple is a technology company.\", {\"entities\": [(0, 5, \"ORG\")]}),\n",
    "]\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "print(\"Before training: \", [(ent.text, ent.label_) for ent in nlp(test).ents])\n",
    "\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "\n",
    "optimizer = nlp.create_optimizer()\n",
    "\n",
    "# Shuffle training data and the dataset using random package per epoch\n",
    "for i in range(epochs):\n",
    "  random.shuffle(training_data)\n",
    "  for text, annotations in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    # Update nlp model after setting sgd argument to optimizer\n",
    "    example = Example.from_dict(doc, annotations)\n",
    "    nlp.update([example], sgd = optimizer)\n",
    "\n",
    "print(\"After training: \", [(ent.text, ent.label_) for ent in nlp(test).ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8963dad-f9ad-4721-b6c3-81517b849adf",
   "metadata": {},
   "source": [
    "#### 3. Save and load a trained model\n",
    "\n",
    "After we have trained a model, the next step is to test the model. For this purpose, we need to save and later load it. \n",
    "\n",
    "- We use the .get_pipe() method to get the trained pipeline component. In an example, if we trained an NER model and hence we get the NER component and save to the disk using the ner.to_disk() method, passing a model name.\n",
    "\n",
    "- Later, we load a spaCy model and create a blank NER component by using the nlp.create_pipe() method.\n",
    "\n",
    "- Then, we load the trained NER model from the disk by using ner.from_disk() method on the created NER component.\n",
    "\n",
    "- Lastly, we add the loaded NER component to the pipeline by calling nlp.add_pipe() method and passing a name for the NER model, such as \"ner\".\n",
    "\n",
    "- Once a trained model is saved, it can be loaded as nlp. Then, we can use the model to find entities of a given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d47efc7d-82d8-406c-8b61-d6b03715783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': np.float32(5.571428)}\n",
      "{'ner': np.float32(11.218441)}\n",
      "{'ner': np.float32(19.2313)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nehaverma/Downloads/ENTER/lib/python3.12/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Elon Musk is the CEO of SpaceX.\" with entities \"[(0, 9, 'PERSON'), (25, 31, 'ORG')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load a blank English model, add NER component, add given labels to the ner pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "\n",
    "# Add labels to the NER component\n",
    "labels = [\"ORG\", \"PERSON\", \"GPE\"]  # Example labels, replace with your labels\n",
    "for ent in labels:\n",
    "    ner.add_label(ent)\n",
    "\n",
    "# Training data (replace with your actual training data)\n",
    "training_data = [\n",
    "    (\"Google was founded in 1998.\", {\"entities\": [(0, 6, \"ORG\")]}),\n",
    "    (\"Elon Musk is the CEO of SpaceX.\", {\"entities\": [(0, 9, \"PERSON\"), (25, 31, \"ORG\")]}),\n",
    "    (\"London is a city in the United Kingdom.\", {\"entities\": [(0, 6, \"GPE\")]}),\n",
    "]\n",
    "\n",
    "for ent in labels:\n",
    "    ner.add_label(ent)\n",
    "\n",
    "# Disable other pipeline components, complete training loop and run training loop\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "nlp.disable_pipes(*other_pipes)\n",
    "losses = {}\n",
    "optimizer = nlp.begin_training()\n",
    "for text, annotation in training_data:\n",
    "    doc = nlp.make_doc(text)\n",
    "    example = Example.from_dict(doc, annotation)\n",
    "    nlp.update([example], sgd=optimizer, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63953ff-6a4a-4546-94fa-1ea39da83722",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
